---
layout: post
title: Discussion- Racist Policing Algorithms
subtitle: Due September 27, 2020
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/thumb.png
share-img: /assets/img/path.jpg
tags: [discussion]
---

# Discussion: Racist Policing Algorithms!

#### What is the government’s stated goal in using predictive policing algorithms?
Location-based algorithms use links between places, events, and historical crime rates tp predict where and when crimes are more likely to happen. The most common algorithm, PredPol, is used by dozens of cities in the United States.

There are also tools that uses ones age, gender, martial status, history of substance abuse, and criminal record to predict who has a high chance of being involved in future crimes. COMPAS, a person-based tool used by juridictions, help make decisions about pretrial released and sentencing. 

"Police like the idea of tools that give them a heads-up and allow them to intervene early because they think it keeps crime rates down."

#### What are some issues that have resulted from the use of these algorithms, and why are they occuring?
Even though in theroy, using AI to predict crime is helpful, predictive algorithms are skewed by biased arrest rates. For example, accordnig to the US Department of Justuce, you are more than twice as likely to be arrested as an black citixen compared to your white counterparts. Even further, a black person is five times as likely to be stopped without just cause than a white person. 

Looking at the Edison Senior High students, students arrested will now deal with biased assessment because of a single arrest. The article descibe this phenomenon of biased arrests of young black people saying, "The data generated by
their arrests would have been fed into algorithms that would disproportionately target all young Black people the algorithms assessed. Though by law the algorithms do not use race as a predictor, other variables, such as socioeconomic background, education, and zip code, act as proxies. Even without explicitly considering race, these tools are racist."

The article also introduces the idea that predictive policing itself is a problem. Dorthy Roberts makes a great point saying, “Racism has always been about predicting, about making certain racial groups seem as if they are predisposed to do bad things and therefore justify controlling them.” This is interesting to me since it explains how predictive policing, specifically machine learning, contributes to systemic racism.  

As more and more cities are using AI, it has become evident that human biases have been "baked" into preditive policing. As Katy Weathington says,“We took bad data in the first place, and then we used tools to make it worse.”

The data used for these algorithms are biased. If biased data is used to train the AI, of course the AI will be biased,

#### What is being done about these issues?
One solution that has risen is the manipulation of data. According to the article, "Black. They found that the best balance between races was achieved when algorithms took race explicitly into account—which existing tools are legally forbidden from doing—and assigned Black people a higher threshold than whites for being deemed high risk." This would be done to balance out biased arrest rates.

Another effort activists are pushing are more transparency between law enforcmenet and the people they serve in terms of how these tool work. In the article it is said that, "The lack of awareness can be blamed on the murkiness of the overall picture: law enforcement has been so tight-lipped about how it uses these technologies that it’s very hard for anyone to assess how well they work. Even when information is available, it is hard to link any one system to any one outcome." Not only is it hard to evaluate these tools, police departments are not doing effort to ensure these tools work properly.

#### Respond to the article. This can be a question, dis/agreement, something surprising, a point of confusion, criticism, a connection to your personal experiences, etc. 
This article is not surprising. I argee with the points are made. However, I think the best solution for fighting bias in AI is the diversification of programmers. If the people who made these algorithms came from different backgrounds, different things would be taken into consideration. For example, the article mentions how police AI disproportionately labels black people are targets because the data of arrests are skewed. I liked how the article mentioned a group of coders who took the bias of arrest records into account. This was probably because there were black people working on this program.

Law enforcement is just another part of how systemic racism hold black people down in every facet of our lives. Now, in a technological age where AI influences our decision-making, we must make it a top priotiy to ensure the AI is objective. "It is not our fault the AI is biased," Yes it is. AI itself isnt prejudice, the people who make the AI are. With this in mind I really hope more PoCs, especially black people are brought into Silicon Valley, because as AI becomes more prevelant we need to access how racism seeps into every industry.
